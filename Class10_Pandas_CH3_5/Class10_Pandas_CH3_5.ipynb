{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Datasets: Concat and Append\n",
    "* Concatenation function: pd.concat\n",
    "    * Concatenate pandas objects along a particular axis with optional set logic\n",
    "along the other axes\n",
    "    * pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "        * objs : a sequence or mapping of Series, DataFrame, or Panel objects\n",
    "            *If a dict is passed, the sorted keys will be used as the `keys` argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised\n",
    "        * axis : {0/'index', 1/'columns'}, default 0\n",
    "            * The axis to concatenate along\n",
    "        * join : {'inner', 'outer'}, default 'outer'\n",
    "            * How to handle indexes on other axis(es)\n",
    "        * join_axes : list of Index objects\n",
    "            * Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logic\n",
    "        * ignore_index : boolean, default False\n",
    "            * If True, do not use the index values along the concatenation axis. \n",
    "            * The resulting axis will be labeled 0, ..., n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.\n",
    "        * keys : sequence, default None\n",
    "            * If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level\n",
    "        * levels : list of sequences, default None\n",
    "            * Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys\n",
    "        * names : list, default None\n",
    "            * Names for the levels in the resulting hierarchical index\n",
    "        * verify_integrity : boolean, default False\n",
    "            * Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation\n",
    "        * copy : boolean, default True\n",
    "            * If False, do not copy data unnecessarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "           for c in cols}\n",
    "    return pd.DataFrame(data, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of DataFrame\n",
    "make_df('ABC', range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall: Concatenation of NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the contents of two or more arrays into a single array:\n",
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "np.concatenate([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes an axis keyword \n",
    "# that allows you to specify the axis \n",
    "# along which the result will be concatenated:\n",
    "x = [[1, 2],\n",
    "     [3, 4]]\n",
    "np.concatenate([x, x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Concetenation with pd.concat\n",
    "* **pd.concat** (objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "keys=None, levels=None, names=None, verify_integrity=False,\n",
    "copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple concatenations of arrays\n",
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate higher-dimensional objects, such as DataFrames\n",
    "df1 = make_df('AB', [1, 2])\n",
    "df2 = make_df('AB', [3, 4])\n",
    "print(df1); print(df2); print(pd.concat([df1, df2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "print(df3); print(df4); print(pd.concat([df3, df4], axis = 'columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate indices\n",
    "* Padans concetenation *preserves indices*, even if the result will have duplicae indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = make_df('AB', [0, 1])\n",
    "y = make_df('AB', [2, 3])\n",
    "y.index = x.index # make duplicate indices!\n",
    "print(x); print(y);print(pd.concat([x, y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([x, y], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catching the repeats as an error\n",
    "* If you’d like to simply verify that the indices in the result of pd.concat() do not overlap, you can specify the verify_integrity flag.\n",
    "* With this set to True, the concatenation will raise an exception if there are duplicate indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pd.concat([x, y], verify_integrity=True)\n",
    "except ValueError as e:\n",
    "    print(\"ValueError\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignoring the index\n",
    "* ignore_index : boolean, default False\n",
    "    * If True, do not use the index values along the concatenation axis.\n",
    "    * The resulting axis will be labeled 0, ..., n - 1. \n",
    "    * This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. \n",
    "    * Note the index values on the other axes are still respected in the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring the indexing\n",
    "print(pd.concat([x, y], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding MultiIndex keys\n",
    "* keys : sequence, default None\n",
    "    * If multiple levels passed, should contain tuples. \n",
    "    * Construct hierarchical index using the passed keys as the outermost level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x); print(y);print(pd.concat([x, y], keys = ['x', 'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation with joins\n",
    "* join : {'inner', 'outer'}, default 'outer'\n",
    "    * How to handle indexes on other axis(es)\n",
    "* join_axes : list of Index objects\n",
    "    * Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "print(df5); print(df6);print(pd.concat([df5, df6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([df5, df6], join = 'inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify that the returned columns should be the same \n",
    "# as those of the first input\n",
    "print(pd.concat([df5, df6], join_axes = [df5.columns]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The append() method\n",
    "* df5.append(other, ignore_index=False, verify_integrity=False)\n",
    "    * Unlike the *append()* and *extend()* methods of Python lists, the *append()* method in Pandas does not modify the original - instead, it creates a new object with the combined data.\n",
    "    * It also is not a very efficient method, because it involves creation of a new index and data buffer.\n",
    "    * better to use concat() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less efficient\n",
    "df = pd.DataFrame(columns=['A'])\n",
    "for i in range(5):\n",
    "    df = df.append({'A': i}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient\n",
    "pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n",
    "          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Datasets: Merge and Join\n",
    "* pd.merge\n",
    "    * Merge DataFrame objects by performing a database-style join operation by columns or indexes\n",
    "    * If joining columns on columns, the DataFrame indexes *will be ignored*. \n",
    "    * Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on.\n",
    "\n",
    "* <font color = red> pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None) </font>\n",
    "\n",
    "    * left: df\n",
    "    * right: df\n",
    "    * how: {'left', 'right', 'outer', 'inner'}, default 'inner'\n",
    "        * left: use only keys from left frame, similar to a SQL left outer join;\n",
    "          preserve key order\n",
    "        * right: use only keys from right frame, similar to a SQL right outer join; preserve key order\n",
    "        * outer: use union of keys from both frames, similar to a SQL full outer\n",
    "          join; sort keys lexicographically\n",
    "        * inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys\n",
    "    * on : label or list\n",
    "        * Field names to join on. \n",
    "        * Must be found in both DataFrames. \n",
    "        * If on is None and not merging on indexes, then it merges on the intersection of the columns by default.\n",
    "    * left_on : label or list, or array-like\n",
    "        * Field names to join on in left DataFrame. \n",
    "        * Can be a vector or list of vectors of the length of the DataFrame to use a particular vector as the join key instead of columns\n",
    "    * right_on : label or list, or array-like\n",
    "        * Field names to join on in right DataFrame or vector/list of vectors per left_on docs  \n",
    "    * left_index : boolean, default False\n",
    "        * Use the index from the left DataFrame as the join key(s). \n",
    "        * If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels\n",
    "    * right_index : boolean, default False \n",
    "        * Use the index from the right DataFrame as the join key. \n",
    "        * Same caveats as left_index\n",
    "    * sort : boolean, default False\n",
    "        * Sort the join keys lexicographically in the result DataFrame. \n",
    "        * If False, the order of the join keys depends on the join type (how keyword)\n",
    "    * suffixes : 2-length sequence (tuple, list, ...)\n",
    "        * Suffix to apply to overlapping column names in the left and right\n",
    "        side, respectively\n",
    "    * copy : boolean, default True\n",
    "        * If False, do not copy data unnecessarily\n",
    "    * indicator : boolean or string, default False\n",
    "        * If True, adds a column to output DataFrame called \"_merge\" with\n",
    "        information on the source of each row.\n",
    "        * If string, column with information on source of each row will be added to output DataFrame, and column will be named value of string.\n",
    "        * Information column is Categorical-type and takes on a value of \"left_only\" for observations whose merge key only appears in 'left' DataFrame, \"right_only\" for observations whose merge key only appears in 'right' DataFrame, and \"both\" if the observation's merge key is found in both.\n",
    "\n",
    "        .. versionadded:: 0.17.0\n",
    "\n",
    "    * validate : string, default None\n",
    "        * If specified, checks if merge is of specified type.\n",
    "\n",
    "        * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n",
    "          left and right datasets.\n",
    "        * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n",
    "          dataset.\n",
    "        * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n",
    "          dataset.\n",
    "        * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n",
    "\n",
    "        .. versionadded:: 0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Algebra\n",
    "### Categories of Joins\n",
    "* *one-to-one*, *many-to-one*, *many-to-many* joins\n",
    "\n",
    "#### One-to-one joins\n",
    "* column-wise concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df3\n",
    "# The pd.merge() function recognizes that each DataFrame has an “employee” column,\n",
    "# and automatically joins using this column as a key. \n",
    "# The result of the merge is a new DataFrame that combines the information \n",
    "# from the two inputs. \n",
    "# Notice that the order of entries in each column is not necessarily maintained: in this case, the order of the\n",
    "# “employee” column differs between df1 and df2, \n",
    "# and the pd.merge() function correctly accounts for this. \n",
    "# Additionally, keep in mind that the merge in general \n",
    "# discards the index, except in the special case of merges by index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-one joins\n",
    "* One of the two key columns contains duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "print(df3); print(df4); print(pd.merge(df3, df4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-many joins\n",
    "* The key column in both the left and right array contains duplicates, then the result is a many-to-many merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "'spreadsheets', 'organization']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1); print(df5); print(pd.merge(df1, df5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of the Merge Key\n",
    "\n",
    "#### The on keyword\n",
    "* on : label or list\n",
    "    * Field names to join on.\n",
    "    * <mark>Must be found in both DataFrames.</mark>\n",
    "    * If on is None and not merging on indexes, then it merges on the intersection of the columns by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1); print(df2);print(pd.merge(df1, df2, on = 'employee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The left_on and right_on keywords\n",
    "* At times you may wish to merge two datasets with different column names; \n",
    "* for example, we may have a dataset in which the employee name\n",
    "* is labeled as “name” rather than “employee”. In this case,\n",
    "* we can use the left_on and right_on keywords to specify the two column names\n",
    "* <font color = red> drop redundant column using .drop() method. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "print(df1); print(df3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df3, left_on = 'employee', right_on = 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df3, left_on = 'employee', right_on = 'name').drop('name', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The left_index and right_index keywords\n",
    "* merge on an index\n",
    "* <font color = red> DataFrames implement the join() method, which performs a merge that defaults to joining on indices </font>\n",
    "* <font color = red> mix indices and columns, combine **left_index with right_on** or **left_on with right_index** to get the desired behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1a = df1.set_index('employee')\n",
    "df2a = df2.set_index('employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1a, df2a, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames implement the join() method, \n",
    "# which performs a merge that defaults to joining on indices\n",
    "df1a.join(df2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix indices and columns, combine left_index with right_on \n",
    "# or left_on with right_index to get the desired behavior\n",
    "pd.merge(df1a, df3, left_index = True, right_on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Set Arithmetic for Joins\n",
    "* how: {'left', 'right', 'outer', 'inner'}, default 'inner'\n",
    "    * left: use only keys from left frame, similar to a SQL left outer join; preserve key order\n",
    "    * right: use only keys from right frame, similar to a SQL right outer join; preserve key order\n",
    "    * outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically\n",
    "    * inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                   columns=['name', 'food'])\n",
    "df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "print(df6); print(df7); pd.merge(df6, df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df7, df6, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df7, df6, how = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping Column Names: The suffixes Keyword\n",
    "* Two input dfs have conflicting column names.\n",
    "* suffixes : 2-length sequence (tuple, list, ...)\n",
    "    * Suffix to apply to overlapping column names in the left and right side, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [1, 2, 3, 4]})\n",
    "df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [3, 1, 4, 2]})\n",
    "print(df8); print(df9); pd.merge(df8, df9, on = 'name')\n",
    "# Because the output would have two conflicting column names, \n",
    "# the merge function automatically appends a suffix_x or _y \n",
    "# to make output columns unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specify a customer suffic using the **suffixes** keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df8, df9, on = 'name', suffixes=['_df8', '_df9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: US States Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are shell commands to download the data\n",
    "!curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv\n",
    "!curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv\n",
    "!curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv('state-population.csv')\n",
    "areas = pd.read_csv('state-areas.csv')\n",
    "abbrevs = pd.read_csv('state-abbrevs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank US states and territories by their 2010 population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use how='outer' to make sure no data is thrown away due to mismatched labels\n",
    "merged = pd.merge(pop, abbrevs, how = 'outer',\n",
    "                 left_on = 'state/region', right_on = 'abbreviation')\n",
    "# drop duplicate ino\n",
    "merged = merged.drop('abbreviation', 1) \n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[ merged['population'].isnull() ]\n",
    "# all the null population values are from Puerto Rico prior to the year\n",
    "2000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the new state entries are also null, \n",
    "# which means that there was no corresponding entry in the abbrevs key! \n",
    "# Let’s figure out which regions lack this match\n",
    "merged.loc[merged['state'].isnull(), 'state/region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico'\n",
    "merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the null population values are from Puerto Rico prior to the year\n",
    "2000;\n",
    "merged.loc[merged['population'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[(merged['year'] < 2000) & (merged['state/region'] == 'PR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the areas df\n",
    "final = pd.merge(merged, areas, on = 'state', how = 'left')\n",
    "final.head(n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null\n",
    "final.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state_pop_2010 = state_pop[state_pop['year']==2010 ]\n",
    "state_pop_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.loc[final['area (sq. mi)'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.loc[:, 'area (sq. mi)'][final['area (sq. mi)'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['state'][final['area (sq. mi)'].isnull()].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usarea = final['area (sq. mi)'].dropna().unique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.loc[:, 'area (sq. mi)'][final['area (sq. mi)'].isnull()].fillna(usarea, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final['state'] == 'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2010 = final.query(\"year == 2010 & ages == 'total'\")\n",
    "data2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2010.set_index('state', inplace = True)\n",
    "density = data2010['population'] / data2010['area (sq. mi)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density.sort_values(ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density.head(n = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
